{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentencePiece VS Huggingface tokenizer\n",
    "\n",
    "한국어 서브워드 분절 알고리즘 실습&비교\n",
    "\n",
    "201229\n",
    "\n",
    "고우영"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 학습시간 비교(s) | 8000   | 16000 | 32000 | 64000 | 128000|\n",
    "|------|------|------|------|------|------|\n",
    "|   Sentencepiece  | 11| 22 |48| 110 |282|\n",
    "|   hugging_face  | 10| 11 |11| 12 |12|\n",
    "\n",
    "| 추론시간 비교(s) | 8000| 128000|\n",
    "|------|------|------|\n",
    "|   Sentencepiece  | 4.5| 4.93 |\n",
    "|   hugging_face  | 4.9| 4.97 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSMC 데이터셋 로드\n",
    "## 15만 문장, 113만 word(띄어쓰기 기준), 평균 7.5word/sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading done!\n",
      "문장: ['아 더빙.. 진짜 짜증나네요 목소리', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '너무재밓었다그래서보는것을추천한다']\n",
      "라벨: [0, 1, 0]\n",
      "['아 더빙.. 진짜 짜증나네요 목소리', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '너무재밓었다그래서보는것을추천한다']\n",
      "\n",
      "코퍼스 문장수/평균/총 단어 갯수 : 149996, 7.6 / 1137736\n",
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# NSMC 데이터 로드\n",
    "import pandas as pd\n",
    "f_train = pd.read_csv('data/nsmc.txt', sep='\\t')\n",
    "train_pair = [(row[1], row[2]) for _, row in f_train.iterrows() if type(row[1]) == str]  # nan 제거\n",
    "\n",
    "#  문장 및 라벨 데이터 추출\n",
    "train_data  = [pair[0] for pair in train_pair]\n",
    "train_label = [pair[1] for pair in train_pair]\n",
    "print('data loading done!')\n",
    "print('문장: %s' %(train_data[:3]))\n",
    "print('라벨: %s' %(train_label[:3]))\n",
    "\n",
    "# subword 학습을 위해 문장만 따로 저장\n",
    "with open('data/train_tokenizer.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in train_data:\n",
    "        f.write(line+'\\n')\n",
    "\n",
    "# subword 학습을 위해 문장만 따로 저장\n",
    "with open('data/train_tokenizer.txt', 'r', encoding='utf-8') as f:\n",
    "    test_tokenizer = f.read().split('\\n')\n",
    "print(test_tokenizer[:3])\n",
    "\n",
    "num_word_list = [len(sentence.split()) for sentence in test_tokenizer]\n",
    "print('\\n코퍼스 문장수/평균/총 단어 갯수 : %d, %.1f / %d' % (len(num_word_list), sum(num_word_list)/len(num_word_list), sum(num_word_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentencePiece 학습"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 설치\n",
    "!pip install sentencepiece==0.1.83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done\n",
      "Wall time: 22.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# spm_train --input=data/train_tokenizer.txt  --model_prefix=sentencepiece/sp --vocab_size=32000 character_coverage=1.0 --model_type=\"unigram\"\n",
    "\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "input_file = 'data/train_tokenizer.txt'\n",
    "vocab_size = 32000\n",
    "sp_model_root='sentencepiece'\n",
    "if not os.path.isdir(sp_model_root):\n",
    "    os.mkdir(sp_model_root)\n",
    "sp_model_name = 'tokenizer_%d' % (vocab_size)\n",
    "sp_model_path = os.path.join(sp_model_root, sp_model_name)\n",
    "model_type = 'unigram'  # bpe\n",
    "character_coverage  = 1.0  # 0.9995\n",
    "user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS],[UNK0],[UNK1],[UNK2],[UNK3],[UNK4],[UNK5],[UNK6],[UNK7],[UNK8],[UNK9],[unused0],[unused1],[unused2],[unused3],[unused4],[unused5],[unused6],[unused7],[unused8],[unused9],[unused10],[unused11],[unused12],[unused13],[unused14],[unused15],[unused16],[unused17],[unused18],[unused19],[unused20],[unused21],[unused22],[unused23],[unused24],[unused25],[unused26],[unused27],[unused28],[unused29],[unused30],[unused31],[unused32],[unused33],[unused34],[unused35],[unused36],[unused37],[unused38],[unused39],[unused40],[unused41],[unused42],[unused43],[unused44],[unused45],[unused46],[unused47],[unused48],[unused49],[unused50],[unused51],[unused52],[unused53],[unused54],[unused55],[unused56],[unused57],[unused58],[unused59],[unused60],[unused61],[unused62],[unused63],[unused64],[unused65],[unused66],[unused67],[unused68],[unused69],[unused70],[unused71],[unused72],[unused73],[unused74],[unused75],[unused76],[unused77],[unused78],[unused79],[unused80],[unused81],[unused82],[unused83],[unused84],[unused85],[unused86],[unused87],[unused88],[unused89],[unused90],[unused91],[unused92],[unused93],[unused94],[unused95],[unused96],[unused97],[unused98],[unused99], [unused100],[unused101],[unused102],[unused103],[unused104],[unused105],[unused106],[unused107],[unused108],[unused109],[unused110],[unused111],[unused112],[unused113],[unused114],[unused115],[unused116],[unused117],[unused118],[unused119],[unused120],[unused121],[unused122],[unused123],[unused124],[unused125],[unused126],[unused127],[unused128],[unused129],[unused130],[unused131],[unused132],[unused133],[unused134],[unused135],[unused136],[unused137],[unused138],[unused139],[unused140],[unused141],[unused142],[unused143],[unused144],[unused145],[unused146],[unused147],[unused148],[unused149],[unused150],[unused151],[unused152],[unused153],[unused154],[unused155],[unused156],[unused157],[unused158],[unused159],[unused160],[unused161],[unused162],[unused163],[unused164],[unused165],[unused166],[unused167],[unused168],[unused169],[unused170],[unused171],[unused172],[unused173],[unused174],[unused175],[unused176],[unused177],[unused178],[unused179],[unused180],[unused181],[unused182],[unused183],[unused184],[unused185],[unused186],[unused187],[unused188],[unused189],[unused190],[unused191],[unused192],[unused193],[unused194],[unused195],[unused196],[unused197],[unused198],[unused199]'\n",
    "input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --user_defined_symbols=%s --model_type=%s --character_coverage=%s'\n",
    "cmd = input_argument%(input_file, sp_model_path, vocab_size, user_defined_symbols, model_type, character_coverage)\n",
    "\n",
    "spm.SentencePieceTrainer.Train(cmd)\n",
    "print('train done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[426, 658, 6112, 3260, 131, 8423, 140, 120]\n",
      "['▁나는', '▁오늘', '▁아침', '밥', '을', '▁먹었', '다', '.']\n",
      "나는 오늘 아침밥을 먹었다.\n",
      "나는 오늘 아침밥을 먹었다.\n"
     ]
    }
   ],
   "source": [
    "## check\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('{}.model'.format(sp_model_path))\n",
    "\n",
    "tokens = sp.encode_as_pieces('나는 오늘 아침밥을 먹었다.')\n",
    "ids = sp.encode_as_ids('나는 오늘 아침밥을 먹었다.')\n",
    "\n",
    "print(ids)\n",
    "print(tokens)\n",
    "\n",
    "tokens = sp.decode_pieces(tokens)\n",
    "ids = sp.decode_ids(ids)\n",
    "\n",
    "print(ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Huggingface setup"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# hugging face tokenizer 설치\n",
    "!pip uninstall tokenizers\n",
    "!pip install tokenizers==0.9.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Huggingface train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertWordPieceTokenizer\n",
      "idx   : [6046, 6200, 7718, 4283, 3269, 1567, 5690, 225]\n",
      "tokens: ['나는', '오늘', '아침', '##밥', '##을', '먹', '##었다', '.']\n",
      "offset: [(0, 2), (3, 5), (6, 8), (8, 9), (9, 10), (11, 12), (12, 14), (14, 15)]\n",
      "idx   : [26007, 5786, 12757, 225, 225, 5916, 6576, 5754, 225, 225, 5672, 13705]\n",
      "tokens: ['교도소', '이야기', '##구먼', '.', '.', '솔직히', '재미는', '없다', '.', '.', '평점', '조정']\n",
      "offset: [(0, 3), (4, 7), (7, 9), (10, 11), (11, 12), (12, 15), (16, 19), (20, 22), (22, 23), (23, 24), (24, 26), (27, 29)]\n",
      "Wall time: 6.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tokenizers import SentencePieceBPETokenizer, BertWordPieceTokenizer\n",
    "\n",
    "how_to_tokenize = BertWordPieceTokenizer\n",
    "# how_to_tokenize = SentencePieceBPETokenizer\n",
    "\n",
    "# Initialize a tokenizer\n",
    "if str(how_to_tokenize) == str(BertWordPieceTokenizer):\n",
    "    print('BertWordPieceTokenizer')\n",
    "    tokenizer = how_to_tokenize(\n",
    "        strip_accents=False,  # Must be False if cased model\n",
    "        lowercase=False,\n",
    "    )\n",
    "elif str(how_to_tokenize) == str(SentencePieceBPETokenizer):\n",
    "    print('SentencePieceBPETokenizer')\n",
    "    tokenizer = how_to_tokenize()\n",
    "else:\n",
    "    assert('select right tokenizer')\n",
    "\n",
    "#########################################\n",
    "corpus_file   = ['data/train_tokenizer.txt']  # data path\n",
    "vocab_size    = 32000\n",
    "limit_alphabet= 6000\n",
    "output_path   = 'hugging_%d'%(vocab_size)\n",
    "\n",
    "hf_model_root='huggingface'\n",
    "if not os.path.isdir(hf_model_root):\n",
    "    os.mkdir(hf_model_root)\n",
    "hf_model_name = 'tokenizer_%d.json' % (vocab_size)\n",
    "hf_model_path = os.path.join(hf_model_root, hf_model_name)\n",
    "\n",
    "min_frequency = 5\n",
    "\n",
    "special_tokens=['[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]',\n",
    "                '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]',\n",
    "                '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]',\n",
    "                '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]',\n",
    "                '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]',\n",
    "                '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]',\n",
    "                '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]',\n",
    "                '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]',\n",
    "                '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]',\n",
    "                '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]',\n",
    "                '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]',\n",
    "                '[unused100]', '[unused101]', '[unused102]', '[unused103]', '[unused104]', '[unused105]', '[unused106]', '[unused107]', '[unused108]', '[unused109]',\n",
    "                '[unused110]', '[unused111]', '[unused112]', '[unused113]', '[unused114]', '[unused115]', '[unused116]', '[unused117]', '[unused118]', '[unused119]',\n",
    "                '[unused120]', '[unused121]', '[unused122]', '[unused123]', '[unused124]', '[unused125]', '[unused126]', '[unused127]', '[unused128]', '[unused129]',\n",
    "                '[unused130]', '[unused131]', '[unused132]', '[unused133]', '[unused134]', '[unused135]', '[unused136]', '[unused137]', '[unused138]', '[unused139]',\n",
    "                '[unused140]', '[unused141]', '[unused142]', '[unused143]', '[unused144]', '[unused145]', '[unused146]', '[unused147]', '[unused148]', '[unused149]',\n",
    "                '[unused150]', '[unused151]', '[unused152]', '[unused153]', '[unused154]', '[unused155]', '[unused156]', '[unused157]', '[unused158]', '[unused159]',\n",
    "                '[unused160]', '[unused161]', '[unused162]', '[unused163]', '[unused164]', '[unused165]', '[unused166]', '[unused167]', '[unused168]', '[unused169]',\n",
    "                '[unused170]', '[unused171]', '[unused172]', '[unused173]', '[unused174]', '[unused175]', '[unused176]', '[unused177]', '[unused178]', '[unused179]',\n",
    "                '[unused180]', '[unused181]', '[unused182]', '[unused183]', '[unused184]', '[unused185]', '[unused186]', '[unused187]', '[unused188]', '[unused189]',\n",
    "                '[unused190]', '[unused191]', '[unused192]', '[unused193]', '[unused194]', '[unused195]', '[unused196]', '[unused197]', '[unused198]', '[unused199]'\n",
    "               ]  # 스페셜 토큰\n",
    "\n",
    "# Then train it!\n",
    "tokenizer.train(files=corpus_file,\n",
    "               vocab_size=vocab_size,\n",
    "               min_frequency=min_frequency,  # 단어의 최소 발생 빈도, 5\n",
    "               limit_alphabet=limit_alphabet,\n",
    "               show_progress=True,\n",
    "               special_tokens=special_tokens\n",
    "               )\n",
    "\n",
    "# And finally save it somewhere\n",
    "tokenizer.save(hf_model_path)\n",
    "\n",
    "output = tokenizer.encode(\"나는 오늘 아침밥을 먹었다.\")\n",
    "print('idx   : %s'%output.ids)\n",
    "print('tokens: %s'%output.tokens)\n",
    "print('offset: %s'%output.offsets)\n",
    "\n",
    "output = tokenizer.encode(\"교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\")\n",
    "print('idx   : %s'%output.ids)\n",
    "print('tokens: %s'%output.tokens)\n",
    "print('offset: %s'%output.offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Huggingface Tokenize test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx   : [6046, 6200, 7718, 4283, 3269, 1567, 5690, 225]\n",
      "tokens: ['나는', '오늘', '아침', '##밥', '##을', '먹', '##었다', '.']\n",
      "offset: [(0, 2), (3, 5), (6, 8), (8, 9), (9, 10), (11, 12), (12, 14), (14, 15)]\n",
      "idx   : [26007, 5786, 12757, 225, 225, 5916, 6576, 5754, 225, 225, 5672, 13705]\n",
      "tokens: ['교도소', '이야기', '##구먼', '.', '.', '솔직히', '재미는', '없다', '.', '.', '평점', '조정']\n",
      "offset: [(0, 3), (4, 7), (7, 9), (10, 11), (11, 12), (12, 15), (16, 19), (20, 22), (22, 23), (23, 24), (24, 26), (27, 29)]\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "how_to_tokenize = BertWordPieceTokenizer\n",
    "# how_to_tokenize = SentencePieceBPETokenizer\n",
    "vocab_size    = 32000\n",
    "\n",
    "tokenizer = Tokenizer.from_file(hf_model_path)\n",
    "\n",
    "\n",
    "output = tokenizer.encode(\"나는 오늘 아침밥을 먹었다.\")\n",
    "print('idx   : %s'%output.ids)\n",
    "print('tokens: %s'%output.tokens)\n",
    "print('offset: %s'%output.offsets)\n",
    "\n",
    "output = tokenizer.encode(\"교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\")\n",
    "print('idx   : %s'%output.ids)\n",
    "print('tokens: %s'%output.tokens)\n",
    "print('offset: %s'%output.offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenze usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SentencePiece Usage, load & 분절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁아', '▁더빙', '..', '▁진짜', '▁짜증나네요', '▁목소리']\n",
      "['▁흠', '...', '포스터보고', '▁초딩영화', '줄', '....', '오버', '연기', '조차', '▁가볍지', '▁않', '구나']\n",
      "['▁너무', '재', '밓', '었다', '그래서', '보는것', '을', '추천', '한다']\n",
      "Wall time: 80 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('{}.model'.format(sp_model_path))\n",
    "sentencepiece_tokenizer = sp.encode_as_pieces\n",
    "\n",
    "result_tokenized_sentencepiece = [sentencepiece_tokenizer(_tmp) for _tmp in test_tokenizer[:3]]\n",
    "    \n",
    "for tmp in result_tokenized_sentencepiece[:3]:\n",
    "    print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Huggingface Usage, load & 분절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아', '더빙', '.', '.', '진짜', '짜증나네요', '목소리']\n",
      "['흠', '.', '.', '.', '포스터보고', '초딩영화', '##줄', '.', '.', '.', '.', '오버', '##연기', '##조차', '가볍지', '않', '##구나']\n",
      "['너무', '##재', '##밓', '##었다', '##그래서', '##보는', '##것을', '##추천', '##한다']\n",
      "Wall time: 285 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "how_to_tokenize = BertWordPieceTokenizer\n",
    "# how_to_tokenize = SentencePieceBPETokenizer\n",
    "vocab_size    = 32000\n",
    "\n",
    "tokenizer = Tokenizer.from_file(hf_model_path)\n",
    "\n",
    "result_tokenized_sentencepiece = [tokenizer.encode(_tmp).tokens for _tmp in test_tokenizer[:3]]\n",
    "for tmp in result_tokenized_sentencepiece[:3]:\n",
    "    print(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
